{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    ">Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#imports\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.nn.functional import cosine_similarity, adaptive_avg_pool2d, softmax\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from functools import partial\n",
    "from losses import CenterLoss\n",
    "# from axa_hw2p2.losses import CenterLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Conv2dAuto(nn.Conv2d):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.padding =  (self.kernel_size[0] // 2, \n",
    "                         self.kernel_size[1] // 2) # dynamic add padding based on the kernel_size\n",
    "        \n",
    "conv3x3 = partial(Conv2dAuto, kernel_size=3, bias=False)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "conv3x3 = partial(Conv2dAuto, kernel_size=3, bias=False)\n",
    "conv = conv3x3(in_channels=32, out_channels=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels: int,\n",
    "                 out_channels: int):\n",
    "        super().__init__()\n",
    "        self.in_channels, self.out_channels =  in_channels, out_channels\n",
    "        self.blocks = nn.Identity()\n",
    "        self.activation_f = nn.ReLU(inplace=True)\n",
    "        self.shortcut = nn.Identity()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        if self.should_apply_shortcut: residual = self.shortcut(x)\n",
    "        x = self.blocks(x)\n",
    "        x += residual\n",
    "        return x\n",
    "    \n",
    "    @property\n",
    "    def should_apply_shortcut(self):\n",
    "        return self.in_channels != self.out_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ResidualBlock(32, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ResNetResidualBlock(ResidualBlock):\n",
    "    def __init__(self, \n",
    "                 in_channels: int, \n",
    "                 out_channels: int, \n",
    "                 expansion=1, \n",
    "                 downsampling=1, \n",
    "                 conv=conv3x3, \n",
    "                 *args, \n",
    "                 **kwargs):\n",
    "        super().__init__(in_channels=in_channels, \n",
    "                         out_channels=out_channels, \n",
    "                         *args, \n",
    "                         **kwargs)\n",
    "        self.expansion = expansion\n",
    "        self.downsampling = downsampling\n",
    "        self.conv = conv\n",
    "\n",
    "        if self.should_apply_shortcut:\n",
    "            self.shortcut = nn.Sequential(nn.Conv2d(self.in_channels, \n",
    "                                                    self.expanded_channels, \n",
    "                                                    kernel_size=1,\n",
    "                                                    stride=self.downsampling, \n",
    "                                                    bias=False),\n",
    "                                          nn.BatchNorm2d(self.expanded_channels)) \n",
    "        else: None        \n",
    "        \n",
    "    @property\n",
    "    def expanded_channels(self):\n",
    "        return self.out_channels * self.expansion\n",
    "    \n",
    "    @property\n",
    "    def should_apply_shortcut(self):\n",
    "        return self.in_channels != self.expanded_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ResNetResidualBlock(32, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def conv_bn(in_channels:int, \n",
    "            out_channels: int, \n",
    "            conv, \n",
    "            *args, \n",
    "            **kwargs):\n",
    "\n",
    "    return nn.Sequential(conv(in_channels, out_channels, *args, **kwargs), nn.BatchNorm2d(out_channels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ResNetBasicBlock(ResNetResidualBlock):\n",
    "    \"\"\"\n",
    "    Basic ResNet block composed by two layers of 3x3conv/batchnorm/activation\n",
    "    \"\"\"\n",
    "    expansion = 1\n",
    "    def __init__(self, \n",
    "                 in_channels: int, \n",
    "                 out_channels: int, \n",
    "                 *args, \n",
    "                 **kwargs):\n",
    "        super().__init__(in_channels=in_channels, \n",
    "                         out_channels=out_channels, \n",
    "                         *args, \n",
    "                         **kwargs)\n",
    "\n",
    "        self.blocks = nn.Sequential(\n",
    "            conv_bn(in_channels=self.in_channels, \n",
    "                    out_channels=self.out_channels, \n",
    "                    conv=self.conv, \n",
    "                    bias=False, \n",
    "                    stride=self.downsampling),\n",
    "            self.activation_f,\n",
    "            conv_bn(in_channels=self.out_channels, \n",
    "                    out_channels=self.expanded_channels, \n",
    "                    conv=self.conv, \n",
    "                    bias=False),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = torch.ones((1, 32, 224, 224))\n",
    "\n",
    "block = ResNetBasicBlock(32, 64)\n",
    "block(dummy).shape\n",
    "print(block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ResNetBottleNeckBlock(ResNetResidualBlock):\n",
    "    expansion = 4\n",
    "    def __init__(self, \n",
    "                 in_channels: int, \n",
    "                 out_channels: int, \n",
    "                 *args, \n",
    "                 **kwargs):\n",
    "        super().__init__(in_channels=in_channels, \n",
    "                         out_channels=out_channels, \n",
    "                         expansion=4, \n",
    "                         *args, \n",
    "                         **kwargs)\n",
    "        self.blocks = nn.Sequential(\n",
    "           conv_bn(in_channels=self.in_channels, \n",
    "                   out_channels=self.out_channels, \n",
    "                   conv=self.conv,\n",
    "                   kernel_size=1),\n",
    "           self.activation_f,\n",
    "           conv_bn(in_channels=self.out_channels, \n",
    "                   out_channels=self.out_channels, \n",
    "                   conv=self.conv, \n",
    "                   kernel_size=3, \n",
    "                   stride=self.downsampling),\n",
    "           self.activation_f,\n",
    "           conv_bn(in_channels=self.out_channels, \n",
    "                   out_channels=self.expanded_channels, \n",
    "                   conv=self.conv, \n",
    "                   kernel_size=1),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = torch.ones((1, 32, 10, 10))\n",
    "\n",
    "block = ResNetBottleNeckBlock(32, 64)\n",
    "block(dummy).shape\n",
    "print(block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ResNetLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    A ResNet layer composed by `n` blocks stacked one after the other\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 in_channels: int, \n",
    "                 out_channels: int, \n",
    "                 block=ResNetBasicBlock, \n",
    "                 n_blocks=1, \n",
    "                 *args, \n",
    "                 **kwargs):\n",
    "        super().__init__()\n",
    "        # 'We perform downsampling directly by convolutional layers that have a stride of 2.'\n",
    "        if in_channels != out_channels: downsampling = 2\n",
    "        else: downsampling = 1\n",
    "        \n",
    "        self.blocks = nn.Sequential(\n",
    "            block(in_channels=in_channels , \n",
    "                  out_channels=out_channels, \n",
    "                  *args, \n",
    "                  **kwargs, \n",
    "                  downsampling=downsampling),\n",
    "            *[block(in_channels=out_channels * block.expansion, \n",
    "                    out_channels=out_channels, \n",
    "                    downsampling=1, \n",
    "                    *args, \n",
    "                    **kwargs) for _ in range(n_blocks - 1)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.blocks(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = torch.ones((1, 64, 48, 48))\n",
    "\n",
    "layer = ResNetLayer(64, 128, block=ResNetBasicBlock, n_blocks=3)\n",
    "layer(dummy).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ResNetEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    ResNet encoder composed by layers with increasing features.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 in_channels=3, \n",
    "                 blocks_sizes=[64, 128, 256, 512], \n",
    "                 deepths=[2,2,2,2], \n",
    "                 block=ResNetBasicBlock, \n",
    "                 *args, \n",
    "                 **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.blocks_sizes = blocks_sizes\n",
    "        \n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels, \n",
    "                      out_channels=self.blocks_sizes[0], \n",
    "                      kernel_size=7, \n",
    "                      stride=2, \n",
    "                      padding=3, \n",
    "                      bias=False),\n",
    "            nn.BatchNorm2d(self.blocks_sizes[0]),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, \n",
    "                         stride=2, \n",
    "                         padding=1)\n",
    "        )\n",
    "        \n",
    "        self.in_out_block_sizes = list(zip(blocks_sizes, blocks_sizes[1:]))\n",
    "\n",
    "        self.blocks = nn.ModuleList([ \n",
    "            ResNetLayer(in_channels=blocks_sizes[0], \n",
    "                        out_channels=blocks_sizes[0], \n",
    "                        n_blocks=deepths[0], \n",
    "                        block=block,\n",
    "                        *args, \n",
    "                        **kwargs),\n",
    "            *[ResNetLayer(in_channels=in_channels * block.expansion, \n",
    "                          out_channels=out_channels, \n",
    "                          n_blocks=n, \n",
    "                          block=block, \n",
    "                          *args, \n",
    "                          **kwargs) \n",
    "              for (in_channels, out_channels), n in zip(self.in_out_block_sizes, deepths[1:])]       \n",
    "        ])\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.gate(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ResnetDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    This class represents the tail of ResNet. It performs a global pooling and maps the output to the\n",
    "    correct class by using a fully connected layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 in_features: int, \n",
    "                 n_classes: int):\n",
    "        super().__init__()\n",
    "        self.avg = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.decoder = nn.Linear(in_features=in_features, \n",
    "                                 out_features=n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.avg(x)\n",
    "        embeddings = x.view(x.size(0), -1)\n",
    "        x = self.decoder(embeddings)\n",
    "        return embeddings, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class _ResNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 in_channels: int, \n",
    "                 n_classes: int, \n",
    "                 *args, \n",
    "                 **kwargs):\n",
    "        super().__init__()\n",
    "        self.encoder = ResNetEncoder(in_channels=in_channels, \n",
    "                                     *args, \n",
    "                                     **kwargs)\n",
    "\n",
    "        n_features = self.encoder.blocks[-1].blocks[-1].expanded_channels\n",
    "        self.decoder = ResnetDecoder(in_features=n_features,\n",
    "                                     n_classes=n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        embeddings, x = self.decoder(x)\n",
    "        return embeddings, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def resnet18(in_channels: int, \n",
    "             n_classes: int, \n",
    "             block=ResNetBasicBlock, \n",
    "             *args, \n",
    "             **kwargs):\n",
    "\n",
    "    return _ResNet(in_channels=in_channels, \n",
    "                   n_classes=n_classes, \n",
    "                   block=block, \n",
    "                   deepths=[2, 2, 2, 2], \n",
    "                   *args, \n",
    "                   **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def resnet34(in_channels: int, \n",
    "             n_classes: int, \n",
    "             block=ResNetBasicBlock, \n",
    "             *args, \n",
    "             **kwargs):\n",
    "\n",
    "    return _ResNet(in_channels=in_channels, \n",
    "                   n_classes=n_classes, \n",
    "                   block=block, \n",
    "                   deepths=[3, 4, 6, 3], \n",
    "                   *args, \n",
    "                   **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def resnet50(in_channels: int, \n",
    "             n_classes: int, \n",
    "             block=ResNetBottleNeckBlock, \n",
    "             *args, \n",
    "             **kwargs):\n",
    "\n",
    "    return _ResNet(in_channels=in_channels, \n",
    "                   n_classes=n_classes, \n",
    "                   block=block, \n",
    "                   deepths=[3, 4, 6, 3], \n",
    "                   *args, \n",
    "                   **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1          [-1, 64, 12, 112]           9,408\n       BatchNorm2d-2          [-1, 64, 12, 112]             128\n              ReLU-3          [-1, 64, 12, 112]               0\n         MaxPool2d-4            [-1, 64, 6, 56]               0\n            Conv2d-5           [-1, 256, 6, 56]          16,384\n       BatchNorm2d-6           [-1, 256, 6, 56]             512\n        Conv2dAuto-7            [-1, 64, 6, 56]           4,096\n       BatchNorm2d-8            [-1, 64, 6, 56]             128\n              ReLU-9            [-1, 64, 6, 56]               0\n             ReLU-10            [-1, 64, 6, 56]               0\n       Conv2dAuto-11            [-1, 64, 6, 56]          36,864\n      BatchNorm2d-12            [-1, 64, 6, 56]             128\n             ReLU-13            [-1, 64, 6, 56]               0\n             ReLU-14            [-1, 64, 6, 56]               0\n       Conv2dAuto-15           [-1, 256, 6, 56]          16,384\n      BatchNorm2d-16           [-1, 256, 6, 56]             512\nResNetBottleNeckBlock-17           [-1, 256, 6, 56]               0\n       Conv2dAuto-18            [-1, 64, 6, 56]          16,384\n      BatchNorm2d-19            [-1, 64, 6, 56]             128\n             ReLU-20            [-1, 64, 6, 56]               0\n             ReLU-21            [-1, 64, 6, 56]               0\n       Conv2dAuto-22            [-1, 64, 6, 56]          36,864\n      BatchNorm2d-23            [-1, 64, 6, 56]             128\n             ReLU-24            [-1, 64, 6, 56]               0\n             ReLU-25            [-1, 64, 6, 56]               0\n       Conv2dAuto-26           [-1, 256, 6, 56]          16,384\n      BatchNorm2d-27           [-1, 256, 6, 56]             512\nResNetBottleNeckBlock-28           [-1, 256, 6, 56]               0\n       Conv2dAuto-29            [-1, 64, 6, 56]          16,384\n      BatchNorm2d-30            [-1, 64, 6, 56]             128\n             ReLU-31            [-1, 64, 6, 56]               0\n             ReLU-32            [-1, 64, 6, 56]               0\n       Conv2dAuto-33            [-1, 64, 6, 56]          36,864\n      BatchNorm2d-34            [-1, 64, 6, 56]             128\n             ReLU-35            [-1, 64, 6, 56]               0\n             ReLU-36            [-1, 64, 6, 56]               0\n       Conv2dAuto-37           [-1, 256, 6, 56]          16,384\n      BatchNorm2d-38           [-1, 256, 6, 56]             512\nResNetBottleNeckBlock-39           [-1, 256, 6, 56]               0\n      ResNetLayer-40           [-1, 256, 6, 56]               0\n           Conv2d-41           [-1, 512, 3, 28]         131,072\n      BatchNorm2d-42           [-1, 512, 3, 28]           1,024\n       Conv2dAuto-43           [-1, 128, 6, 56]          32,768\n      BatchNorm2d-44           [-1, 128, 6, 56]             256\n             ReLU-45           [-1, 128, 6, 56]               0\n             ReLU-46           [-1, 128, 6, 56]               0\n       Conv2dAuto-47           [-1, 128, 3, 28]         147,456\n      BatchNorm2d-48           [-1, 128, 3, 28]             256\n             ReLU-49           [-1, 128, 3, 28]               0\n             ReLU-50           [-1, 128, 3, 28]               0\n       Conv2dAuto-51           [-1, 512, 3, 28]          65,536\n      BatchNorm2d-52           [-1, 512, 3, 28]           1,024\nResNetBottleNeckBlock-53           [-1, 512, 3, 28]               0\n       Conv2dAuto-54           [-1, 128, 3, 28]          65,536\n      BatchNorm2d-55           [-1, 128, 3, 28]             256\n             ReLU-56           [-1, 128, 3, 28]               0\n             ReLU-57           [-1, 128, 3, 28]               0\n       Conv2dAuto-58           [-1, 128, 3, 28]         147,456\n      BatchNorm2d-59           [-1, 128, 3, 28]             256\n             ReLU-60           [-1, 128, 3, 28]               0\n             ReLU-61           [-1, 128, 3, 28]               0\n       Conv2dAuto-62           [-1, 512, 3, 28]          65,536\n      BatchNorm2d-63           [-1, 512, 3, 28]           1,024\nResNetBottleNeckBlock-64           [-1, 512, 3, 28]               0\n       Conv2dAuto-65           [-1, 128, 3, 28]          65,536\n      BatchNorm2d-66           [-1, 128, 3, 28]             256\n             ReLU-67           [-1, 128, 3, 28]               0\n             ReLU-68           [-1, 128, 3, 28]               0\n       Conv2dAuto-69           [-1, 128, 3, 28]         147,456\n      BatchNorm2d-70           [-1, 128, 3, 28]             256\n             ReLU-71           [-1, 128, 3, 28]               0\n             ReLU-72           [-1, 128, 3, 28]               0\n       Conv2dAuto-73           [-1, 512, 3, 28]          65,536\n      BatchNorm2d-74           [-1, 512, 3, 28]           1,024\nResNetBottleNeckBlock-75           [-1, 512, 3, 28]               0\n       Conv2dAuto-76           [-1, 128, 3, 28]          65,536\n      BatchNorm2d-77           [-1, 128, 3, 28]             256\n             ReLU-78           [-1, 128, 3, 28]               0\n             ReLU-79           [-1, 128, 3, 28]               0\n       Conv2dAuto-80           [-1, 128, 3, 28]         147,456\n      BatchNorm2d-81           [-1, 128, 3, 28]             256\n             ReLU-82           [-1, 128, 3, 28]               0\n             ReLU-83           [-1, 128, 3, 28]               0\n       Conv2dAuto-84           [-1, 512, 3, 28]          65,536\n      BatchNorm2d-85           [-1, 512, 3, 28]           1,024\nResNetBottleNeckBlock-86           [-1, 512, 3, 28]               0\n      ResNetLayer-87           [-1, 512, 3, 28]               0\n           Conv2d-88          [-1, 1024, 2, 14]         524,288\n      BatchNorm2d-89          [-1, 1024, 2, 14]           2,048\n       Conv2dAuto-90           [-1, 256, 3, 28]         131,072\n      BatchNorm2d-91           [-1, 256, 3, 28]             512\n             ReLU-92           [-1, 256, 3, 28]               0\n             ReLU-93           [-1, 256, 3, 28]               0\n       Conv2dAuto-94           [-1, 256, 2, 14]         589,824\n      BatchNorm2d-95           [-1, 256, 2, 14]             512\n             ReLU-96           [-1, 256, 2, 14]               0\n             ReLU-97           [-1, 256, 2, 14]               0\n       Conv2dAuto-98          [-1, 1024, 2, 14]         262,144\n      BatchNorm2d-99          [-1, 1024, 2, 14]           2,048\nResNetBottleNeckBlock-100          [-1, 1024, 2, 14]               0\n      Conv2dAuto-101           [-1, 256, 2, 14]         262,144\n     BatchNorm2d-102           [-1, 256, 2, 14]             512\n            ReLU-103           [-1, 256, 2, 14]               0\n            ReLU-104           [-1, 256, 2, 14]               0\n      Conv2dAuto-105           [-1, 256, 2, 14]         589,824\n     BatchNorm2d-106           [-1, 256, 2, 14]             512\n            ReLU-107           [-1, 256, 2, 14]               0\n            ReLU-108           [-1, 256, 2, 14]               0\n      Conv2dAuto-109          [-1, 1024, 2, 14]         262,144\n     BatchNorm2d-110          [-1, 1024, 2, 14]           2,048\nResNetBottleNeckBlock-111          [-1, 1024, 2, 14]               0\n      Conv2dAuto-112           [-1, 256, 2, 14]         262,144\n     BatchNorm2d-113           [-1, 256, 2, 14]             512\n            ReLU-114           [-1, 256, 2, 14]               0\n            ReLU-115           [-1, 256, 2, 14]               0\n      Conv2dAuto-116           [-1, 256, 2, 14]         589,824\n     BatchNorm2d-117           [-1, 256, 2, 14]             512\n            ReLU-118           [-1, 256, 2, 14]               0\n            ReLU-119           [-1, 256, 2, 14]               0\n      Conv2dAuto-120          [-1, 1024, 2, 14]         262,144\n     BatchNorm2d-121          [-1, 1024, 2, 14]           2,048\nResNetBottleNeckBlock-122          [-1, 1024, 2, 14]               0\n      Conv2dAuto-123           [-1, 256, 2, 14]         262,144\n     BatchNorm2d-124           [-1, 256, 2, 14]             512\n            ReLU-125           [-1, 256, 2, 14]               0\n            ReLU-126           [-1, 256, 2, 14]               0\n      Conv2dAuto-127           [-1, 256, 2, 14]         589,824\n     BatchNorm2d-128           [-1, 256, 2, 14]             512\n            ReLU-129           [-1, 256, 2, 14]               0\n            ReLU-130           [-1, 256, 2, 14]               0\n      Conv2dAuto-131          [-1, 1024, 2, 14]         262,144\n     BatchNorm2d-132          [-1, 1024, 2, 14]           2,048\nResNetBottleNeckBlock-133          [-1, 1024, 2, 14]               0\n      Conv2dAuto-134           [-1, 256, 2, 14]         262,144\n     BatchNorm2d-135           [-1, 256, 2, 14]             512\n            ReLU-136           [-1, 256, 2, 14]               0\n            ReLU-137           [-1, 256, 2, 14]               0\n      Conv2dAuto-138           [-1, 256, 2, 14]         589,824\n     BatchNorm2d-139           [-1, 256, 2, 14]             512\n            ReLU-140           [-1, 256, 2, 14]               0\n            ReLU-141           [-1, 256, 2, 14]               0\n      Conv2dAuto-142          [-1, 1024, 2, 14]         262,144\n     BatchNorm2d-143          [-1, 1024, 2, 14]           2,048\nResNetBottleNeckBlock-144          [-1, 1024, 2, 14]               0\n      Conv2dAuto-145           [-1, 256, 2, 14]         262,144\n     BatchNorm2d-146           [-1, 256, 2, 14]             512\n            ReLU-147           [-1, 256, 2, 14]               0\n            ReLU-148           [-1, 256, 2, 14]               0\n      Conv2dAuto-149           [-1, 256, 2, 14]         589,824\n     BatchNorm2d-150           [-1, 256, 2, 14]             512\n            ReLU-151           [-1, 256, 2, 14]               0\n            ReLU-152           [-1, 256, 2, 14]               0\n      Conv2dAuto-153          [-1, 1024, 2, 14]         262,144\n     BatchNorm2d-154          [-1, 1024, 2, 14]           2,048\nResNetBottleNeckBlock-155          [-1, 1024, 2, 14]               0\n     ResNetLayer-156          [-1, 1024, 2, 14]               0\n          Conv2d-157           [-1, 2048, 1, 7]       2,097,152\n     BatchNorm2d-158           [-1, 2048, 1, 7]           4,096\n      Conv2dAuto-159           [-1, 512, 2, 14]         524,288\n     BatchNorm2d-160           [-1, 512, 2, 14]           1,024\n            ReLU-161           [-1, 512, 2, 14]               0\n            ReLU-162           [-1, 512, 2, 14]               0\n      Conv2dAuto-163            [-1, 512, 1, 7]       2,359,296\n     BatchNorm2d-164            [-1, 512, 1, 7]           1,024\n            ReLU-165            [-1, 512, 1, 7]               0\n            ReLU-166            [-1, 512, 1, 7]               0\n      Conv2dAuto-167           [-1, 2048, 1, 7]       1,048,576\n     BatchNorm2d-168           [-1, 2048, 1, 7]           4,096\nResNetBottleNeckBlock-169           [-1, 2048, 1, 7]               0\n      Conv2dAuto-170            [-1, 512, 1, 7]       1,048,576\n     BatchNorm2d-171            [-1, 512, 1, 7]           1,024\n            ReLU-172            [-1, 512, 1, 7]               0\n            ReLU-173            [-1, 512, 1, 7]               0\n      Conv2dAuto-174            [-1, 512, 1, 7]       2,359,296\n     BatchNorm2d-175            [-1, 512, 1, 7]           1,024\n            ReLU-176            [-1, 512, 1, 7]               0\n            ReLU-177            [-1, 512, 1, 7]               0\n      Conv2dAuto-178           [-1, 2048, 1, 7]       1,048,576\n     BatchNorm2d-179           [-1, 2048, 1, 7]           4,096\nResNetBottleNeckBlock-180           [-1, 2048, 1, 7]               0\n      Conv2dAuto-181            [-1, 512, 1, 7]       1,048,576\n     BatchNorm2d-182            [-1, 512, 1, 7]           1,024\n            ReLU-183            [-1, 512, 1, 7]               0\n            ReLU-184            [-1, 512, 1, 7]               0\n      Conv2dAuto-185            [-1, 512, 1, 7]       2,359,296\n     BatchNorm2d-186            [-1, 512, 1, 7]           1,024\n            ReLU-187            [-1, 512, 1, 7]               0\n            ReLU-188            [-1, 512, 1, 7]               0\n      Conv2dAuto-189           [-1, 2048, 1, 7]       1,048,576\n     BatchNorm2d-190           [-1, 2048, 1, 7]           4,096\nResNetBottleNeckBlock-191           [-1, 2048, 1, 7]               0\n     ResNetLayer-192           [-1, 2048, 1, 7]               0\n   ResNetEncoder-193           [-1, 2048, 1, 7]               0\nAdaptiveAvgPool2d-194           [-1, 2048, 1, 1]               0\n          Linear-195                 [-1, 4000]       8,196,000\n   ResnetDecoder-196   [[-1, 2048], [-1, 4000]]               0\n================================================================\nTotal params: 31,704,032\nTrainable params: 31,704,032\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.06\nForward/backward pass size (MB): 29.91\nParams size (MB): 120.94\nEstimated Total Size (MB): 150.92\n----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "res_net_deepth = 50\n",
    "\n",
    "if res_net_deepth == 18: model = resnet18\n",
    "if res_net_deepth == 34: model = resnet34\n",
    "if res_net_deepth == 50: model = resnet50\n",
    "\n",
    "model = model(in_channels=3, n_classes=4000)\n",
    "summary(model, (3, 24, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ResNetN():\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 res_net_deepth,\n",
    "                 in_channels: int,\n",
    "                 n_classes: int,\n",
    "                 lr: float,\n",
    "                 lr_decay: float,\n",
    "                 n_lr_decay_steps: int,\n",
    "                 center_loss: bool,\n",
    "                 lr_cl: float,\n",
    "                 alpha_cl: float,\n",
    "                 n_epochs: int,\n",
    "                 eval_steps: int):\n",
    "\n",
    "        assert res_net_deepth in [18, 34, 50]\n",
    "\n",
    "        # Architecture parameters\n",
    "        self.in_channels = in_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.center_loss = center_loss\n",
    "        if res_net_deepth == 50: self.n_embeddings = 2048\n",
    "        else: self.n_embeddings = 512\n",
    "\n",
    "        # Optimization parameters\n",
    "        self.lr = lr\n",
    "        self.lr_decay = lr_decay\n",
    "        self.n_lr_decay_steps = n_lr_decay_steps\n",
    "        self.lr_cl = lr_cl\n",
    "        self.alpha_cl = alpha_cl\n",
    "        self.n_epochs = n_epochs\n",
    "        self.eval_steps = eval_steps\n",
    "\n",
    "        if res_net_deepth == 18: self.model = resnet18\n",
    "        if res_net_deepth == 34: self.model = resnet34\n",
    "        if res_net_deepth == 50: self.model = resnet50\n",
    "\n",
    "        self.model = self.model(in_channels=in_channels,\n",
    "                                n_classes=n_classes)\n",
    "\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    def fit(self, train_loader, val_c_loader, val_v_loader):\n",
    "\n",
    "        print(\"=\"*30 + 'Start Fitting' + \"=\"*30)\n",
    "        self.model.to(self.device)\n",
    "        self.model.train()\n",
    "        \n",
    "        cross_entroypy_loss_f = nn.CrossEntropyLoss()\n",
    "        center_loss_f = CenterLoss(num_classes=self.n_classes,\n",
    "                                   feat_dim=self.n_embeddings,\n",
    "                                   use_gpu=torch.cuda.is_available())\n",
    "    \n",
    "        # optimizer = Adam(self.model.parameters(), \n",
    "        #                  lr=self.lr, \n",
    "        #                  weight_decay=0.00004)\n",
    "\n",
    "        # optimizer_centerloss = Adam(center_loss_f.parameters(),\n",
    "        #                             lr=self.lr_cl)\n",
    "\n",
    "        optimizer = SGD(self.model.parameters(), \n",
    "                        lr=self.lr, \n",
    "                        weight_decay=0.00004,\n",
    "                        momentum=0.9)\n",
    "\n",
    "        optimizer_centerloss = SGD(center_loss_f.parameters(),\n",
    "                                   lr=self.lr_cl)                            \n",
    "\n",
    "        scheduler = StepLR(optimizer=optimizer, \n",
    "                           step_size=self.n_epochs//self.n_lr_decay_steps,\n",
    "                           gamma=self.lr_decay)\n",
    "        \n",
    "        self.train_loss = -1\n",
    "        self.val_c_loss = -1\n",
    "        self.train_c_acc = 0\n",
    "        self.val_c_acc = 0\n",
    "        self.val_v_acc = 0\n",
    "        self.trajectories = {'epoch': [],\n",
    "                             'train_loss': [],\n",
    "                             'train_c_acc': [],\n",
    "                             'val_c_loss': [],\n",
    "                             'val_c_acc': [],\n",
    "                             'val_v_acc':[]}\n",
    "\n",
    "        for epoch in range(self.n_epochs):\n",
    "            \n",
    "            train_loss = 0\n",
    "            train_correct_predictions = 0\n",
    "            train_total_predictions = 0\n",
    "\n",
    "            for batch_idx, (img, label) in enumerate(train_loader):\n",
    "                img = img.to(self.device)\n",
    "                label = label.to(self.device)\n",
    "                \n",
    "                embeddings, cl_output = self.model(img)\n",
    "\n",
    "                if self.center_loss == True:\n",
    "                    loss = self.alpha_cl * center_loss_f(embeddings, label) + \\\n",
    "                       cross_entroypy_loss_f(cl_output, label)\n",
    "\n",
    "                else:\n",
    "                    loss = cross_entroypy_loss_f(cl_output, label)\n",
    "\n",
    "                predicted = torch.argmax(cl_output.data, 1)\n",
    "                train_correct_predictions += (predicted == label).sum().item()\n",
    "                train_total_predictions += len(label)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                if self.center_loss == True: \n",
    "                    optimizer_centerloss.zero_grad()\n",
    "\n",
    "                loss.backward()\n",
    "                \n",
    "                if self.center_loss == True:\n",
    "                    for p in center_loss_f.parameters():\n",
    "                        p.grad.data *= (1./self.alpha_cl)\n",
    "                \n",
    "                optimizer.step()\n",
    "\n",
    "                if self.center_loss == True:\n",
    "                    optimizer_centerloss.step()\n",
    "\n",
    "                scheduler.step()\n",
    "\n",
    "                train_loss += loss.item()\n",
    "            \n",
    "            train_loss /= len(train_loader)\n",
    "            train_c_acc = train_correct_predictions/train_total_predictions     \n",
    "\n",
    "            if epoch % self.eval_steps == 0:\n",
    "                val_c_loss, val_c_acc, val_v_acc = \\\n",
    "                    self.evaluate_performance(val_c_loader, \n",
    "                                              val_v_loader)\n",
    "        \n",
    "                self.trajectories['epoch'].append(epoch)\n",
    "                self.trajectories['train_loss'].append(train_loss)\n",
    "                self.trajectories['train_c_acc'].append(train_c_acc)\n",
    "                self.trajectories['val_c_loss'].append(val_c_loss)\n",
    "                self.trajectories['val_c_acc'].append(val_c_acc)\n",
    "                self.trajectories['val_v_acc'].append(val_v_acc)\n",
    "\n",
    "                display_str = f'epoch: {epoch} '\n",
    "                display_str += f'train_loss: {np.round(train_loss,4)} '\n",
    "                display_str += f'train_c_acc: {np.round(train_c_acc,4):.2%} '\n",
    "                display_str += f'val_c_loss: {np.round(val_c_loss,4)} '\n",
    "                display_str += f'val_c_acc: {np.round(val_c_acc,4):.2%} '\n",
    "                display_str += f'val_v_acc: {np.round(val_v_acc,4):.2%} '\n",
    "                print(display_str)\n",
    "\n",
    "                if self.val_c_loss > val_c_loss: self.val_c_loss = val_c_loss\n",
    "                if self.train_loss > train_loss: self.train_loss = train_loss\n",
    "                if self.train_c_acc < train_c_acc: self.train_c_acc = train_c_acc\n",
    "                if self.val_c_acc < val_c_acc: self.val_c_acc = val_c_acc\n",
    "                if self.val_v_acc < val_v_acc: self.val_v_acc = val_v_acc\n",
    "        \n",
    "        print(\"=\"*72+\"\\n\")\n",
    "\n",
    "\n",
    "    def evaluate_performance(self, val_c_loader, val_v_loader):\n",
    "\n",
    "        cross_entroypy_loss_f = nn.CrossEntropyLoss()\n",
    "        center_loss_f = CenterLoss(num_classes=self.n_classes,\n",
    "                                   feat_dim=self.n_embeddings,\n",
    "                                   use_gpu=torch.cuda.is_available())\n",
    "        \n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "        val_c_loss = 0.0\n",
    "        total_predictions = 0.0\n",
    "        correct_predictions = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (img, label) in enumerate(val_c_loader):\n",
    "                img = img.to(self.device)\n",
    "                label = label.to(self.device)\n",
    "\n",
    "                embeddings, cl_output = self.model(img)\n",
    "\n",
    "                if self.center_loss == True:\n",
    "                    loss = self.alpha_cl * center_loss_f(embeddings, label) + \\\n",
    "                       cross_entroypy_loss_f(cl_output, label)\n",
    "\n",
    "                else:\n",
    "                    loss = cross_entroypy_loss_f(cl_output, label)\n",
    "\n",
    "                loss = loss.detach()\n",
    "                val_c_loss += loss.item()\n",
    "\n",
    "                predicted = torch.argmax(cl_output.data, 1)\n",
    "                total_predictions += len(label)\n",
    "                correct_predictions += (predicted == label).sum().item()\n",
    "\n",
    "        val_c_loss /= len(val_c_loader) \n",
    "        val_c_acc = correct_predictions/total_predictions\n",
    "\n",
    "        similarity = np.array([])\n",
    "        ver_bool = np.array([])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (img_0, img_1, target) in enumerate(val_v_loader):\n",
    "                img_0 = img_0.to(self.device)\n",
    "                img_1 = img_1.to(self.device)\n",
    "\n",
    "                emb_0 = self.model(img_0)[0]\n",
    "                emb_1 = self.model(img_1)[0]\n",
    "\n",
    "                sim_score = cosine_similarity(emb_0, emb_1)\n",
    "                similarity = np.append(similarity, sim_score.cpu().numpy().reshape(-1))\n",
    "                ver_bool = np.append(ver_bool, target)\n",
    "            \n",
    "        val_v_acc = roc_auc_score(ver_bool, similarity)\n",
    "\n",
    "        return val_c_loss, val_c_acc, val_v_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from axa_hw2p2.datasets import FaceClassificationDataset, FaceVerificationDataset\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/alejandroxag/myfiles/deeplearning/hw2p2'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()\n",
    "# os.chdir('./hw2p2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_train = np.array(range(6))\n",
    "sample_val_c = list(range(2))\n",
    "sample_val_c = np.array([sample_train[i] for i in sample_val_c])\n",
    "sample_val_v = np.array(range(2))\n",
    "\n",
    "train_dataset = FaceClassificationDataset(sample_train, mode='train')\n",
    "val_c_dataset = FaceClassificationDataset(sample_val_c, mode='val')\n",
    "val_v_dataset = FaceVerificationDataset(sample_val_v, mode='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================================================\nin_channels             3\nn_classes               6\nbatch_size              1\nlr                  0.001\nlr_decay             0.99\nn_lr_decay_steps        4\ncenter_loss          True\nlr_cl                 0.5\nalpha_cl             0.01\nn_epochs               16\neval_steps              4\ndtype: object\n=============================================================================\n"
     ]
    }
   ],
   "source": [
    "# Architecture parameters\n",
    "mc = {}\n",
    "mc['in_channels'] = 3\n",
    "mc['n_classes'] = len(np.unique(train_dataset.labels))\n",
    "\n",
    "# Optimization and regularization parameters\n",
    "mc['batch_size'] = 1\n",
    "mc['lr'] = 0.001\n",
    "mc['lr_decay'] = 0.99\n",
    "mc['n_lr_decay_steps'] = 4\n",
    "mc['center_loss'] = True\n",
    "mc['lr_cl'] = 0.5\n",
    "mc['alpha_cl'] = 0.01\n",
    "mc['n_epochs'] = 16\n",
    "mc['eval_steps'] = 4\n",
    "\n",
    "print(77*'=')\n",
    "print(pd.Series(mc))\n",
    "print(77*'=')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1           [-1, 64, 32, 32]           9,408\n       BatchNorm2d-2           [-1, 64, 32, 32]             128\n              ReLU-3           [-1, 64, 32, 32]               0\n         MaxPool2d-4           [-1, 64, 16, 16]               0\n            Conv2d-5          [-1, 256, 16, 16]          16,384\n       BatchNorm2d-6          [-1, 256, 16, 16]             512\n        Conv2dAuto-7           [-1, 64, 16, 16]           4,096\n       BatchNorm2d-8           [-1, 64, 16, 16]             128\n              ReLU-9           [-1, 64, 16, 16]               0\n             ReLU-10           [-1, 64, 16, 16]               0\n       Conv2dAuto-11           [-1, 64, 16, 16]          36,864\n      BatchNorm2d-12           [-1, 64, 16, 16]             128\n             ReLU-13           [-1, 64, 16, 16]               0\n             ReLU-14           [-1, 64, 16, 16]               0\n       Conv2dAuto-15          [-1, 256, 16, 16]          16,384\n      BatchNorm2d-16          [-1, 256, 16, 16]             512\nResNetBottleNeckBlock-17          [-1, 256, 16, 16]               0\n       Conv2dAuto-18           [-1, 64, 16, 16]          16,384\n      BatchNorm2d-19           [-1, 64, 16, 16]             128\n             ReLU-20           [-1, 64, 16, 16]               0\n             ReLU-21           [-1, 64, 16, 16]               0\n       Conv2dAuto-22           [-1, 64, 16, 16]          36,864\n      BatchNorm2d-23           [-1, 64, 16, 16]             128\n             ReLU-24           [-1, 64, 16, 16]               0\n             ReLU-25           [-1, 64, 16, 16]               0\n       Conv2dAuto-26          [-1, 256, 16, 16]          16,384\n      BatchNorm2d-27          [-1, 256, 16, 16]             512\nResNetBottleNeckBlock-28          [-1, 256, 16, 16]               0\n       Conv2dAuto-29           [-1, 64, 16, 16]          16,384\n      BatchNorm2d-30           [-1, 64, 16, 16]             128\n             ReLU-31           [-1, 64, 16, 16]               0\n             ReLU-32           [-1, 64, 16, 16]               0\n       Conv2dAuto-33           [-1, 64, 16, 16]          36,864\n      BatchNorm2d-34           [-1, 64, 16, 16]             128\n             ReLU-35           [-1, 64, 16, 16]               0\n             ReLU-36           [-1, 64, 16, 16]               0\n       Conv2dAuto-37          [-1, 256, 16, 16]          16,384\n      BatchNorm2d-38          [-1, 256, 16, 16]             512\nResNetBottleNeckBlock-39          [-1, 256, 16, 16]               0\n      ResNetLayer-40          [-1, 256, 16, 16]               0\n           Conv2d-41            [-1, 512, 8, 8]         131,072\n      BatchNorm2d-42            [-1, 512, 8, 8]           1,024\n       Conv2dAuto-43          [-1, 128, 16, 16]          32,768\n      BatchNorm2d-44          [-1, 128, 16, 16]             256\n             ReLU-45          [-1, 128, 16, 16]               0\n             ReLU-46          [-1, 128, 16, 16]               0\n       Conv2dAuto-47            [-1, 128, 8, 8]         147,456\n      BatchNorm2d-48            [-1, 128, 8, 8]             256\n             ReLU-49            [-1, 128, 8, 8]               0\n             ReLU-50            [-1, 128, 8, 8]               0\n       Conv2dAuto-51            [-1, 512, 8, 8]          65,536\n      BatchNorm2d-52            [-1, 512, 8, 8]           1,024\nResNetBottleNeckBlock-53            [-1, 512, 8, 8]               0\n       Conv2dAuto-54            [-1, 128, 8, 8]          65,536\n      BatchNorm2d-55            [-1, 128, 8, 8]             256\n             ReLU-56            [-1, 128, 8, 8]               0\n             ReLU-57            [-1, 128, 8, 8]               0\n       Conv2dAuto-58            [-1, 128, 8, 8]         147,456\n      BatchNorm2d-59            [-1, 128, 8, 8]             256\n             ReLU-60            [-1, 128, 8, 8]               0\n             ReLU-61            [-1, 128, 8, 8]               0\n       Conv2dAuto-62            [-1, 512, 8, 8]          65,536\n      BatchNorm2d-63            [-1, 512, 8, 8]           1,024\nResNetBottleNeckBlock-64            [-1, 512, 8, 8]               0\n       Conv2dAuto-65            [-1, 128, 8, 8]          65,536\n      BatchNorm2d-66            [-1, 128, 8, 8]             256\n             ReLU-67            [-1, 128, 8, 8]               0\n             ReLU-68            [-1, 128, 8, 8]               0\n       Conv2dAuto-69            [-1, 128, 8, 8]         147,456\n      BatchNorm2d-70            [-1, 128, 8, 8]             256\n             ReLU-71            [-1, 128, 8, 8]               0\n             ReLU-72            [-1, 128, 8, 8]               0\n       Conv2dAuto-73            [-1, 512, 8, 8]          65,536\n      BatchNorm2d-74            [-1, 512, 8, 8]           1,024\nResNetBottleNeckBlock-75            [-1, 512, 8, 8]               0\n       Conv2dAuto-76            [-1, 128, 8, 8]          65,536\n      BatchNorm2d-77            [-1, 128, 8, 8]             256\n             ReLU-78            [-1, 128, 8, 8]               0\n             ReLU-79            [-1, 128, 8, 8]               0\n       Conv2dAuto-80            [-1, 128, 8, 8]         147,456\n      BatchNorm2d-81            [-1, 128, 8, 8]             256\n             ReLU-82            [-1, 128, 8, 8]               0\n             ReLU-83            [-1, 128, 8, 8]               0\n       Conv2dAuto-84            [-1, 512, 8, 8]          65,536\n      BatchNorm2d-85            [-1, 512, 8, 8]           1,024\nResNetBottleNeckBlock-86            [-1, 512, 8, 8]               0\n      ResNetLayer-87            [-1, 512, 8, 8]               0\n           Conv2d-88           [-1, 1024, 4, 4]         524,288\n      BatchNorm2d-89           [-1, 1024, 4, 4]           2,048\n       Conv2dAuto-90            [-1, 256, 8, 8]         131,072\n      BatchNorm2d-91            [-1, 256, 8, 8]             512\n             ReLU-92            [-1, 256, 8, 8]               0\n             ReLU-93            [-1, 256, 8, 8]               0\n       Conv2dAuto-94            [-1, 256, 4, 4]         589,824\n      BatchNorm2d-95            [-1, 256, 4, 4]             512\n             ReLU-96            [-1, 256, 4, 4]               0\n             ReLU-97            [-1, 256, 4, 4]               0\n       Conv2dAuto-98           [-1, 1024, 4, 4]         262,144\n      BatchNorm2d-99           [-1, 1024, 4, 4]           2,048\nResNetBottleNeckBlock-100           [-1, 1024, 4, 4]               0\n      Conv2dAuto-101            [-1, 256, 4, 4]         262,144\n     BatchNorm2d-102            [-1, 256, 4, 4]             512\n            ReLU-103            [-1, 256, 4, 4]               0\n            ReLU-104            [-1, 256, 4, 4]               0\n      Conv2dAuto-105            [-1, 256, 4, 4]         589,824\n     BatchNorm2d-106            [-1, 256, 4, 4]             512\n            ReLU-107            [-1, 256, 4, 4]               0\n            ReLU-108            [-1, 256, 4, 4]               0\n      Conv2dAuto-109           [-1, 1024, 4, 4]         262,144\n     BatchNorm2d-110           [-1, 1024, 4, 4]           2,048\nResNetBottleNeckBlock-111           [-1, 1024, 4, 4]               0\n      Conv2dAuto-112            [-1, 256, 4, 4]         262,144\n     BatchNorm2d-113            [-1, 256, 4, 4]             512\n            ReLU-114            [-1, 256, 4, 4]               0\n            ReLU-115            [-1, 256, 4, 4]               0\n      Conv2dAuto-116            [-1, 256, 4, 4]         589,824\n     BatchNorm2d-117            [-1, 256, 4, 4]             512\n            ReLU-118            [-1, 256, 4, 4]               0\n            ReLU-119            [-1, 256, 4, 4]               0\n      Conv2dAuto-120           [-1, 1024, 4, 4]         262,144\n     BatchNorm2d-121           [-1, 1024, 4, 4]           2,048\nResNetBottleNeckBlock-122           [-1, 1024, 4, 4]               0\n      Conv2dAuto-123            [-1, 256, 4, 4]         262,144\n     BatchNorm2d-124            [-1, 256, 4, 4]             512\n            ReLU-125            [-1, 256, 4, 4]               0\n            ReLU-126            [-1, 256, 4, 4]               0\n      Conv2dAuto-127            [-1, 256, 4, 4]         589,824\n     BatchNorm2d-128            [-1, 256, 4, 4]             512\n            ReLU-129            [-1, 256, 4, 4]               0\n            ReLU-130            [-1, 256, 4, 4]               0\n      Conv2dAuto-131           [-1, 1024, 4, 4]         262,144\n     BatchNorm2d-132           [-1, 1024, 4, 4]           2,048\nResNetBottleNeckBlock-133           [-1, 1024, 4, 4]               0\n      Conv2dAuto-134            [-1, 256, 4, 4]         262,144\n     BatchNorm2d-135            [-1, 256, 4, 4]             512\n            ReLU-136            [-1, 256, 4, 4]               0\n            ReLU-137            [-1, 256, 4, 4]               0\n      Conv2dAuto-138            [-1, 256, 4, 4]         589,824\n     BatchNorm2d-139            [-1, 256, 4, 4]             512\n            ReLU-140            [-1, 256, 4, 4]               0\n            ReLU-141            [-1, 256, 4, 4]               0\n      Conv2dAuto-142           [-1, 1024, 4, 4]         262,144\n     BatchNorm2d-143           [-1, 1024, 4, 4]           2,048\nResNetBottleNeckBlock-144           [-1, 1024, 4, 4]               0\n      Conv2dAuto-145            [-1, 256, 4, 4]         262,144\n     BatchNorm2d-146            [-1, 256, 4, 4]             512\n            ReLU-147            [-1, 256, 4, 4]               0\n            ReLU-148            [-1, 256, 4, 4]               0\n      Conv2dAuto-149            [-1, 256, 4, 4]         589,824\n     BatchNorm2d-150            [-1, 256, 4, 4]             512\n            ReLU-151            [-1, 256, 4, 4]               0\n            ReLU-152            [-1, 256, 4, 4]               0\n      Conv2dAuto-153           [-1, 1024, 4, 4]         262,144\n     BatchNorm2d-154           [-1, 1024, 4, 4]           2,048\nResNetBottleNeckBlock-155           [-1, 1024, 4, 4]               0\n     ResNetLayer-156           [-1, 1024, 4, 4]               0\n          Conv2d-157           [-1, 2048, 2, 2]       2,097,152\n     BatchNorm2d-158           [-1, 2048, 2, 2]           4,096\n      Conv2dAuto-159            [-1, 512, 4, 4]         524,288\n     BatchNorm2d-160            [-1, 512, 4, 4]           1,024\n            ReLU-161            [-1, 512, 4, 4]               0\n            ReLU-162            [-1, 512, 4, 4]               0\n      Conv2dAuto-163            [-1, 512, 2, 2]       2,359,296\n     BatchNorm2d-164            [-1, 512, 2, 2]           1,024\n            ReLU-165            [-1, 512, 2, 2]               0\n            ReLU-166            [-1, 512, 2, 2]               0\n      Conv2dAuto-167           [-1, 2048, 2, 2]       1,048,576\n     BatchNorm2d-168           [-1, 2048, 2, 2]           4,096\nResNetBottleNeckBlock-169           [-1, 2048, 2, 2]               0\n      Conv2dAuto-170            [-1, 512, 2, 2]       1,048,576\n     BatchNorm2d-171            [-1, 512, 2, 2]           1,024\n            ReLU-172            [-1, 512, 2, 2]               0\n            ReLU-173            [-1, 512, 2, 2]               0\n      Conv2dAuto-174            [-1, 512, 2, 2]       2,359,296\n     BatchNorm2d-175            [-1, 512, 2, 2]           1,024\n            ReLU-176            [-1, 512, 2, 2]               0\n            ReLU-177            [-1, 512, 2, 2]               0\n      Conv2dAuto-178           [-1, 2048, 2, 2]       1,048,576\n     BatchNorm2d-179           [-1, 2048, 2, 2]           4,096\nResNetBottleNeckBlock-180           [-1, 2048, 2, 2]               0\n      Conv2dAuto-181            [-1, 512, 2, 2]       1,048,576\n     BatchNorm2d-182            [-1, 512, 2, 2]           1,024\n            ReLU-183            [-1, 512, 2, 2]               0\n            ReLU-184            [-1, 512, 2, 2]               0\n      Conv2dAuto-185            [-1, 512, 2, 2]       2,359,296\n     BatchNorm2d-186            [-1, 512, 2, 2]           1,024\n            ReLU-187            [-1, 512, 2, 2]               0\n            ReLU-188            [-1, 512, 2, 2]               0\n      Conv2dAuto-189           [-1, 2048, 2, 2]       1,048,576\n     BatchNorm2d-190           [-1, 2048, 2, 2]           4,096\nResNetBottleNeckBlock-191           [-1, 2048, 2, 2]               0\n     ResNetLayer-192           [-1, 2048, 2, 2]               0\n   ResNetEncoder-193           [-1, 2048, 2, 2]               0\nAdaptiveAvgPool2d-194           [-1, 2048, 1, 1]               0\n          Linear-195                    [-1, 6]          12,294\n   ResnetDecoder-196      [[-1, 2048], [-1, 6]]               0\n================================================================\nTotal params: 23,520,326\nTrainable params: 23,520,326\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.05\nForward/backward pass size (MB): 22.92\nParams size (MB): 89.72\nEstimated Total Size (MB): 112.69\n----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = ResNetN(18,\n",
    "                in_channels=mc['in_channels'],\n",
    "                n_classes=mc['n_classes'],\n",
    "                lr=mc['lr'],\n",
    "                lr_decay=mc['lr_decay'],\n",
    "                n_lr_decay_steps=mc['n_lr_decay_steps'],\n",
    "                center_loss = mc['center_loss'],\n",
    "                lr_cl=mc['lr_cl'],\n",
    "                alpha_cl=mc['alpha_cl'],\n",
    "                n_epochs=mc['n_epochs'],\n",
    "                eval_steps=mc['eval_steps'])\n",
    "\n",
    "summary(model.model, (3, 64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, \n",
    "                          shuffle=True, \n",
    "                          batch_size=mc['batch_size'], \n",
    "                          drop_last=True)\n",
    "\n",
    "val_c_loader = DataLoader(val_c_dataset, \n",
    "                          shuffle=False, \n",
    "                          batch_size=1, \n",
    "                          drop_last=True)\n",
    "                          \n",
    "val_v_loader = DataLoader(val_v_dataset, \n",
    "                          shuffle=False, \n",
    "                          batch_size=1, \n",
    "                          drop_last=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================Start Fitting==============================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-114-340b40e4dca4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m model.fit(train_loader=train_loader,\n\u001b[1;32m      2\u001b[0m           \u001b[0mval_c_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_c_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m           val_v_loader=val_v_loader)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-102-2dfe7b112f7e>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_loader, val_c_loader, val_v_loader)\u001b[0m\n\u001b[1;32m    119\u001b[0m                         \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha_cl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcenter_loss\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/idl_ubuntu/lib/python3.7/site-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/idl_ubuntu/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/idl_ubuntu/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/idl_ubuntu/lib/python3.7/site-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    115\u001b[0m                   \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                   \u001b[0mdampening\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                   nesterov)\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;31m# update momentum_buffers in state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/idl_ubuntu/lib/python3.7/site-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36msgd\u001b[0;34m(params, d_p_list, momentum_buffer_list, weight_decay, momentum, lr, dampening, nesterov)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0md_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md_p_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mweight_decay\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0md_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight_decay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmomentum\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(train_loader=train_loader,\n",
    "          val_c_loader=val_c_loader,\n",
    "          val_v_loader=val_v_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('idl_ubuntu': conda)",
   "metadata": {
    "interpreter": {
     "hash": "90fe67124204a04bb5d130edc44abf452890e25295d02a40ff695cfcd1c7864c"
    }
   },
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
