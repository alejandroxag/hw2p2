# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/face_class_ver.ipynb (unless otherwise specified).

__all__ = ['SIZE', 'NO_TRANSF', 'TRAIN_TRANSF', 'VAL_TRANSF', 'FaceClassificationDataset', 'FaceVerificationDataset',
           'create_dataloaders', 'fit_predict', 'main', 'parse_args']

# Cell
# imports
import os
import time
import pickle
import argparse
from functools import partial
import json
from datetime import datetime

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

import torchvision
from torchvision import transforms
from PIL import Image
from sklearn.metrics import roc_auc_score

from hyperopt import fmin, tpe, hp, Trials, STATUS_OK
from hyperopt.pyll.base import scope

from .models.resnet2 import ResNet

#from hw2p2.datasets import FaceClassificationDataset, FaceVerificationDataset

# Cell
SIZE = 64
NO_TRANSF = [transforms.ToTensor(),
             transforms.Resize(SIZE)]

TRAIN_TRANSF = [transforms.Resize(SIZE),
                #transforms.RandomCrop(64),
                transforms.RandomHorizontalFlip(),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.225, 0.226, 0.225],
                                     std=[0.5, 0.5, 0.5])]

VAL_TRANSF = [transforms.Resize(SIZE),
              transforms.ToTensor(),
              transforms.Normalize(mean=[0.225, 0.226, 0.225],
                                   std=[0.5, 0.5, 0.5])]

# Cell
class FaceClassificationDataset(Dataset):
    """
    """
    def __init__(self,
                 mode,
                 trans_list):

        # Assertions to avoid wrong inputs
        assert mode in ['train', 'val', 'test']
        assert mode == 'test' and n_classes == None or mode != 'test'

        # Directory setup
        data_dirs = {'train': './data/s1/train_data',
                     'val': './data/s1/val_data',
                     'test': './data/s1/test_data'}
        self.data_dir = data_dirs[mode]
        self.mode = mode
        self.trans_list = trans_list

        # Labels
        if (mode in ['train', 'val']):
            #self.labels = np.array([int(d) for d in os.listdir(self.data_dir)])
            self.labels = np.array([int(d) for d in os.listdir(self.data_dir) \
                                           for file in os.listdir(f'{self.data_dir}/{d}') if '.ipynb' not in file])

            self.X = np.array([str(file) for d in os.listdir(self.data_dir) \
                                           for file in os.listdir(f'{self.data_dir}/{d}') if '.ipynb' not in file])

            #self.labels = self.labels[:50_000]
            #self.X = self.X[:50_000]

        elif mode == 'test':
            self.labels = os.listdir(self.data_dir)
            self.labels = np.array([int(f.split('.')[0]) for f in self.labels])
            self.labels.sort(axis=0)

            self.map_files = []
            for l in self.labels:
                temp_ls = [(l, f) for f in \
                    os.listdir(os.path.join(self.data_dir, str(l)))]
                temp_ls = [(t[0], t[1]) for t in temp_ls]
                self.map_files.append(temp_ls)

            self.map_files = [t for sl in self.map_files for t in sl]
            self.labels = [t[0] for t in self.map_files]
            self.X = [t[1] for t in self.map_files]

    def __len__(self):
            return len(self.X)

    def __getitem__(self, idx):
        preprocess = transforms.Compose(self.trans_list)

        image_path = os.path.join(self.data_dir,
                                  str(self.labels[idx]),
                                  self.X[idx])
        image_tensor = Image.open(image_path)
        image_tensor = preprocess(image_tensor)

        if self.mode == 'test':
            return image_tensor
        else:
            return image_tensor, self.labels[idx]

# Cell
class FaceVerificationDataset(Dataset):
    """
    """
    def __init__(self,
                 mode,
                 trans_list):

        # Assertions to avoid wrong inputs
        assert mode in ['val', 'test']
        assert mode == 'test' and sample == None or mode != 'test'

        # Directory setup
        pairs_dirs = {'val': './data/s2/verification_pairs_val.txt',
                     'test': './data/s2/verification_pairs_test.txt'}
        self.pairs_dir = pairs_dirs[mode]
        self.mode = mode
        self.trans_list = trans_list

        with open(self.pairs_dir) as f:
            self.pairs = [l.rstrip().split() for l in f]

    def __len__(self):
        return len(self.pairs)

    def __getitem__(self, idx):
        preprocess = transforms.Compose(self.trans_list)

        image_tensor_0 = Image.open('./data/s2/' + self.pairs[idx][0])
        image_tensor_1 = Image.open('./data/s2/' + self.pairs[idx][1])

        image_tensor_0 = preprocess(image_tensor_0)
        image_tensor_1 = preprocess(image_tensor_1)

        if self.mode == 'test':
            return image_tensor_0, image_tensor_1
        else:
            return image_tensor_0, image_tensor_1, int(self.pairs[idx][2])

# Cell
def create_dataloaders(mc):
    num_workers = 8 if torch.cuda.is_available() else 0
    train_dataset = FaceClassificationDataset(trans_list=TRAIN_TRANSF, mode='train')
    clf_dataset   = FaceClassificationDataset(trans_list=VAL_TRANSF, mode='val')
    vrf_dataset   = FaceVerificationDataset(trans_list=VAL_TRANSF, mode='val')

    print(f'n_train: {len(train_dataset)}, n_clf: {len(clf_dataset)}, n_vrf: {len(vrf_dataset)}')

    train_loader = DataLoader(train_dataset,
                              shuffle=True,
                              batch_size=mc['batch_size'],
                              num_workers=num_workers,
                              pin_memory=torch.cuda.is_available(),
                              drop_last=True)

    clf_loader = DataLoader(clf_dataset,
                            shuffle=False,
                            batch_size=2048,
                            num_workers=num_workers,
                            pin_memory=torch.cuda.is_available(),
                            drop_last=True)

    vrf_loader = DataLoader(vrf_dataset,
                            shuffle=False,
                            batch_size=2048,
                            num_workers=num_workers,
                            pin_memory=torch.cuda.is_available(),
                            drop_last=True)

    assert len(train_loader) > 0
    assert len(clf_loader) > 0
    assert len(vrf_loader) > 0

    return train_loader, clf_loader, vrf_loader

# Cell
def fit_predict(mc, verbose, trials=None):

    train_loader, clf_loader, vrf_loader = create_dataloaders(mc)

    print(f'\nCurrent directory: {os.getcwd()}\n')
    now = datetime.now().strftime("%d-%m-%y_%H-%M-%S")
    print(now)

    start_time = time.time()
    print('='*26)
    print(pd.Series(mc))
    print('='*26+'\n')

    model = ResNet(params=mc)
    model.fit(train_loader=train_loader,
              val_loader=clf_loader,
              vrf_loader=vrf_loader)

    results = {'loss':  1.-model.best_acc,
               'train_loss': model.train_loss,
               'train_acc': model.train_accuracy,
               'val_loss': model.val_loss,
               'val_acc': model.val_c_acc,
               'mc': mc,
               'run_time': time.time()-start_time,
               'trajectories': model.trajectories,
               'time_stamp': now,
               'status': STATUS_OK}
    return results

# Cell
def main(args, max_evals):
    # Hyperparameters space
    space = {'experiment_id': hp.choice(label='input_size', options=[args.experiment_id]),
             'input_size': hp.choice(label='input_size', options=[SIZE]),
             'n_classes': hp.choice(label='n_classes', options=[4000]),
             #'iterations': hp.choice(label='iterations', options=[300_000]), #(n_samples/batch_size) * epochs = (400_000/128) * 100
             'iterations': hp.choice(label='iterations', options=[5_000]),
             'batch_size': scope.int(hp.choice(label='batch_size', options=[128])),
             #'initial_lr': hp.loguniform(label='lr', low=np.log(5e-3), high=np.log(5e-2)),
             'initial_lr': hp.choice(label='lr', options=[0.1]),
             'lr_decay': hp.choice(label='lr_decay', options=[0.5]),
             'adjust_lr_step': hp.choice(label='n_lr_decay_steps', options=[300_000//2]),
             'weight_decay': hp.choice(label='weight_decay', options=[5e-4]),
             'initial_clr': hp.choice(label='initial_clr', options=[0.5]),
             'alpha': hp.choice(label='alpha', options=[0.1, 0.01]),
             'display_step': scope.int(hp.choice(label='eval_epochs', options=[1])),
             #'display_step': scope.int(hp.choice(label='eval_epochs', options=[3_000])),
             'random_seed': scope.int(hp.quniform('random_seed', 1, 10, 1))}

    # Hyperparameters search
    trials = Trials()
    fmin_objective = partial(fit_predict, trials=trials, verbose=True)
    best_model = fmin(fmin_objective, space=space, algo=tpe.suggest, max_evals=max_evals, trials=trials)

    # Save output
    hyperopt_file = './results/trials.p'
    with open(hyperopt_file, "wb") as f:
        pickle.dump(trials, f)

# Cell
def parse_args():
    desc = "Classification/anomaly detection shared trend metric experiment"
    parser = argparse.ArgumentParser(description=desc)
    parser.add_argument('--experiment_id', required=True, type=str, help='string to identify experiment')
    return parser.parse_args()

# Cell
if __name__ == "__main__":
    #args = parse_args()
    args = pd.Series({'experiment_id': 'exp1'})
    main(args, max_evals=2)